#!/usr/bin/env python3
"""
catchup_diagnostic.py: Diagnostic script to identify issues with the catchup process.

This script performs detailed analysis of:
1. Archive data status
2. API connectivity and data availability
3. Date range gaps
4. Validation logic testing

Usage:
    python catchup_diagnostic.py --bucket lookout
"""

import argparse
from datetime import datetime, timedelta

import pandas as pd

import lookout.api.awn_controller as awn
from lookout.api.ambient_client import get_device_history_raw, get_devices
from lookout.utils.log_util import app_logger

logger = app_logger(__name__)


def diagnose_archive(device, bucket_name):
    """Analyze the current archive state."""
    print("\n" + "=" * 50)
    print("ARCHIVE DIAGNOSIS")
    print("=" * 50)

    archive_df = awn.load_archive_for_device(device, bucket_name)

    if archive_df.empty:
        print("âŒ Archive is empty!")
        return None

    print(f"âœ… Archive loaded: {len(archive_df)} records")

    if "dateutc" in archive_df.columns:
        archive_min = pd.to_datetime(archive_df["dateutc"].min(), unit="ms")
        archive_max = pd.to_datetime(archive_df["dateutc"].max(), unit="ms")
        print(f"ğŸ“… Date range: {archive_min} to {archive_max}")

        # Calculate gap to current time
        current_time = datetime.now()
        gap = current_time - archive_max.to_pydatetime()
        print(f"â° Gap to current time: {gap}")

        if gap.days > 0:
            print(f"âš ï¸  Archive is {gap.days} days behind!")
        elif gap.total_seconds() > 3600:
            hours = gap.total_seconds() / 3600
            print(f"âš ï¸  Archive is {hours:.1f} hours behind")
        else:
            print("âœ… Archive is recent")

        # Show last few records
        print("\nğŸ“‹ Last 3 records in archive:")
        last_records = archive_df.tail(3)
        for idx, row in last_records.iterrows():
            ts = pd.to_datetime(row["dateutc"], unit="ms")
            print(f"  {ts}")

    return archive_df


def diagnose_api_data(device, archive_df):
    """Test API data retrieval and compare with archive."""
    print("\n" + "=" * 50)
    print("API DATA DIAGNOSIS")
    print("=" * 50)

    mac = device.get("macAddress")

    # Test basic API connectivity
    print("ğŸ”Œ Testing API connectivity...")
    try:
        raw_data = get_device_history_raw(mac, limit=5)
        if not raw_data:
            print("âŒ No data returned from API")
            return
        print(f"âœ… API returned {len(raw_data)} records")

        # Show latest API record
        latest = raw_data[0]  # Most recent record
        latest_ts = pd.to_datetime(latest["dateutc"], unit="ms")
        print(f"ğŸ“… Latest API record: {latest_ts}")

    except Exception as e:
        print(f"âŒ API error: {e}")
        return

    if archive_df is None or archive_df.empty:
        print("âš ï¸  Cannot compare with archive (empty)")
        return

    # Compare archive max with API latest
    archive_max = pd.to_datetime(archive_df["dateutc"].max(), unit="ms")
    api_latest = pd.to_datetime(latest["dateutc"], unit="ms")

    print("\nğŸ” Comparison:")
    print(f"  Archive max: {archive_max}")
    print(f"  API latest:  {api_latest}")

    if api_latest > archive_max:
        gap = api_latest - archive_max
        print(f"âœ… New data available! Gap: {gap}")
    elif api_latest == archive_max:
        print("âš ï¸  Archive is up to date (no new data)")
    else:
        print("âŒ API data is older than archive?!")


def test_fetch_logic(device, archive_df):
    """Test the specific fetch logic that's failing."""
    print("\n" + "=" * 50)
    print("FETCH LOGIC TESTING")
    print("=" * 50)

    if archive_df is None or archive_df.empty:
        print("âŒ Cannot test fetch logic without archive data")
        return

    # Get the last date from archive
    last_date = pd.to_datetime(archive_df["dateutc"].max(), unit="ms").to_pydatetime()
    print(f"ğŸ“… Starting fetch from: {last_date}")

    # Test the actual fetch function
    print("ğŸ”„ Testing get_device_history_from_date...")
    try:
        new_data = awn.get_device_history_from_date(device, last_date, limit=250)

        if new_data.empty:
            print("âŒ get_device_history_from_date returned empty DataFrame")

            # Try fetching from a slightly earlier date
            earlier_date = last_date - timedelta(hours=1)
            print(f"ğŸ”„ Trying from 1 hour earlier: {earlier_date}")
            new_data = awn.get_device_history_from_date(device, earlier_date, limit=250)

            if new_data.empty:
                print("âŒ Still empty with earlier date")
            else:
                print(f"âœ… Got {len(new_data)} records with earlier date")

        else:
            print(f"âœ… Fetched {len(new_data)} records")

            # Analyze the fetched data
            if "dateutc" in new_data.columns:
                new_min = pd.to_datetime(new_data["dateutc"].min(), unit="ms")
                new_max = pd.to_datetime(new_data["dateutc"].max(), unit="ms")
                print(f"ğŸ“… Fetched range: {new_min} to {new_max}")

                # Check if it's actually new
                archive_max = pd.to_datetime(archive_df["dateutc"].max(), unit="ms")
                if new_max > archive_max:
                    print("âœ… Fetched data is newer than archive")
                else:
                    print("âš ï¸  Fetched data is not newer than archive")
                    print(f"   Archive max: {archive_max}")
                    print(f"   Fetched max: {new_max}")

    except Exception as e:
        print(f"âŒ Error in fetch logic: {e}")


def test_date_calculation(archive_df):
    """Test the date calculation logic."""
    print("\n" + "=" * 50)
    print("DATE CALCULATION TESTING")
    print("=" * 50)

    if archive_df is None or archive_df.empty:
        print("âŒ Cannot test without archive data")
        return

    last_date = pd.to_datetime(archive_df["dateutc"].max(), unit="ms").to_pydatetime()
    print(f"ğŸ“… Archive max date: {last_date}")

    current_time = datetime.now()
    print(f"ğŸ“… Current time: {current_time}")

    # Test the end_date calculation from get_device_history_from_date
    limit = 250
    end_date = last_date + timedelta(minutes=(limit - 3) * 5)

    if end_date > current_time:
        end_date = current_time

    print(f"ğŸ“… Calculated end_date: {end_date}")
    print(f"ğŸ“… Time span being requested: {end_date - last_date}")

    # Check if this makes sense
    if last_date > current_time:
        print("âŒ Last date is in the future!")
    elif end_date <= last_date:
        print("âŒ End date is not after start date!")
    elif (end_date - last_date).total_seconds() < 300:  # Less than 5 minutes
        print("âš ï¸  Very small time window being requested")
    else:
        print("âœ… Date calculation looks reasonable")


def main():
    parser = argparse.ArgumentParser(description="Diagnose catchup issues")
    parser.add_argument("--bucket", type=str, default="lookout", help="S3 bucket name")
    args = parser.parse_args()

    print("ğŸ” CATCHUP DIAGNOSTIC TOOL")
    print("=" * 50)

    # Get device
    devices = get_devices()
    if not devices:
        print("âŒ No devices found")
        return

    device = devices[0]
    mac = device.get("macAddress")
    name = device.get("info", {}).get("name", "Unnamed Device")
    print(f"ğŸ“¡ Device: {name} ({mac})")

    # Run diagnostics
    archive_df = diagnose_archive(device, args.bucket)
    diagnose_api_data(device, archive_df)
    test_fetch_logic(device, archive_df)
    test_date_calculation(archive_df)

    print("\n" + "=" * 50)
    print("DIAGNOSIS COMPLETE")
    print("=" * 50)


if __name__ == "__main__":
    main()
